# -*- coding: utf-8 -*-
"""TPindex

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pij5zv7c6sVXg_PtDW4KFAwUGW0nqL5H
"""

import nltk
nltk.download('book')

##Import
from nltk.tokenize import sent_tokenize, word_tokenize 
from nltk.stem import PorterStemmer
from nltk.corpus import stopwords 
from nltk.text import TextCollection

##Ce sont les mots vides a enlever
stop_words=stopwords.words("english")


##Fonction filtre permet la suppression des mot vides du txt

def filtre(txt) :
  filtered_sentence = []
  word_tokens=word_tokenize(txt)
  for w in word_tokens:
    if w not in stop_words:
      filtered_sentence.append(w)
  return filtered_sentence



##Fonction qui realisation la racinisation des mot filtré
##selon l'algorithme de porter
def racinisation(ftxt):
  racine=[]
  ps = PorterStemmer()
  for w in ftxt:
    racine.append(ps.stem(w))
  return racine


##Fonction qui retourne l'occ du mot dans la list donnée en entrée

def occ(racine,mot):
  all_words = []
  for w in racine:
    all_words.append(w.lower())
  all_words = nltk.FreqDist(all_words)
  return all_words[mot]

##Fonction qui retourne la pertinence d'un mot
def tfidf(racine,mot,i):
  mytexts = TextCollection(racine) ##List des racines de text
  return mytexts.tf_idf(mot,i) #Racine du mot et text racine

##Fonction qui retourne la poids d'un mot
import math
def poids(o,n,df):
  return (1+math.log(o))*math.log(n/df)

##main function (txts:list des texts) 
def main (txts,mot):

  df=0

  ##Racinisation du mot
  racine_mot=racinisation([mot])[0]
  
  ##Chaine de racine
  S_racine=[]
  ##Realiser les etape d'indexation et recherche

  for i in range(len(txts)):

    f=filtre(txts[i])
    r=racinisation(f)
    o=occ(r,racine_mot)
    if o>0: 
      df=df+1
      S_racine.append([r,i+1,o])## S_racine list de format : [[racine du txt,numero du text,occurence du mot dans le text],...]
    
  ##Si le mot existe dans au moin un document
  ##On calcul le poids du mot dans chaque doc
  if df!=0:

    pp={"poids":-1}##Inisialisation du doc le plus pertinent a -1
    doc=[]##Doc list de format : [{"text":numero(i),"Occurence du mot":occ,"Poids":pd,"Pertinence(tfidf)":tfidf},...]
    
    for racine in S_racine:
      obj={"text":racine[1],"occurence":racine[2],"poids":poids(racine[2],len(txts),df),"Pertinence(tfidf)":tfidf(S_racine,racine_mot,racine[0])}
      if pp["poids"]<obj["poids"]:##Verif du doc le plus pertinent
        pp=obj
      doc.append(obj)  
    print("la liste des doc contenant :'",mot,"'est :\n",doc)## Affichage du num de text dans la list,occurence(mot),poids
                                                             ## des doc ou se trouve le mot
    if pp["poids"]==0:
      print("Tous les docs de méme pertinence 0")
    else :
      print("le doc le plus pertinent est :\n", pp)
  else :
    print( "Le mot n'est pas present dans aucun doc")

from nltk.corpus import state_union ,gutenberg

#Import de plusieur txt de corpus 
#sur lequel on vas faire une indexation

text1 = state_union.raw("2005-GWBush.txt")
text2 = gutenberg.raw("austen-emma.txt");
text3 = gutenberg.raw("whitman-leaves.txt")
text4 = state_union.raw("1990-Bush.txt")
text5=gutenberg.raw("edgeworth-parents.txt")

##Liste des texts a indexée
txts=[text1,text2,text3,text4,text5]


##Test

main(txts,"guilt")

#main(txts,"war")

#main(txts,"war")

#main(txts,"eidolon")